{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data & PreProcessing\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    print(\"Libraries Loaded Successfully\")\n",
    "except ImportError:\n",
    "    print(\"Failed to Load Libraries!\")\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, path=r'C:\\Users\\NoteBook\\Desktop\\alphabet\\DS-3', image_size=50, shrink=0, padding=10, threshold=100, invert=False):\n",
    "        self.PATH = path\n",
    "        self.IMAGE_SIZE = image_size\n",
    "        self.PADDING = padding\n",
    "        self.INVERT = invert\n",
    "        self.THRESHOLD = threshold\n",
    "        self.SLICE = shrink\n",
    "\n",
    "        self.x_data = []\n",
    "        self.y_data = []\n",
    "        self.labels = []\n",
    "        self.CATEGORIES = []\n",
    "        self.list_categories = []\n",
    "\n",
    "    def get_categories(self):\n",
    "        \"\"\"Get and sort categories based on folder names.\"\"\"\n",
    "        for folder in os.listdir(self.PATH):\n",
    "            label = folder.split(\"-\")[0]\n",
    "            self.labels.append(label)\n",
    "            self.list_categories.append(folder)\n",
    "\n",
    "        try:\n",
    "            self.list_categories = sorted(self.list_categories, key=lambda x: int(x.split(\"-\")[0]))\n",
    "        except ValueError:\n",
    "            self.list_categories = sorted(self.list_categories)\n",
    "\n",
    "        print(\"Found Categories:\", self.list_categories, '\\n')\n",
    "        return self.list_categories\n",
    "\n",
    "    def centerize(self, image):\n",
    "        \"\"\"Center the letter in the image using contours to find the bounding box.\"\"\"\n",
    "\n",
    "        contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if len(contours) == 0:\n",
    "            return image\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n",
    "\n",
    "        cropped_image = image[y:y + h, x:x + w]\n",
    "\n",
    "        centered_image = np.zeros((self.IMAGE_SIZE, self.IMAGE_SIZE), dtype=np.uint8)\n",
    "\n",
    "        start_x = (self.IMAGE_SIZE - w) // 2\n",
    "        start_y = (self.IMAGE_SIZE - h) // 2\n",
    "\n",
    "        centered_image[start_y:start_y + h, start_x:start_x + w] = cropped_image\n",
    "\n",
    "        return centered_image\n",
    "\n",
    "    def enhance(self, image):\n",
    "        \"\"\"Enhance the quality of image by boosting the values below a certain threshold\"\"\"\n",
    "\n",
    "        _, enhanced_image = cv2.threshold(image, self.THRESHOLD, 255, cv2.THRESH_BINARY)\n",
    "        return enhanced_image\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"Preprocess the image by zooming in (shrinking) while keeping the final size constant.\"\"\"\n",
    "\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return None\n",
    "\n",
    "        padded_image = cv2.copyMakeBorder(image, self.PADDING, self.PADDING, self.PADDING, self.PADDING, cv2.BORDER_CONSTANT, value=0)\n",
    "        if self.INVERT:\n",
    "            padded_image = 255 - padded_image\n",
    "\n",
    "        resized_image = cv2.resize(padded_image, (self.IMAGE_SIZE, self.IMAGE_SIZE))\n",
    "\n",
    "        cropped_image = resized_image[self.SLICE:self.IMAGE_SIZE - self.SLICE,\n",
    "                        self.SLICE:self.IMAGE_SIZE - self.SLICE]\n",
    "\n",
    "        zoomed_image = cv2.resize(cropped_image, (self.IMAGE_SIZE, self.IMAGE_SIZE))\n",
    "\n",
    "        centered_image = self.centerize(zoomed_image)\n",
    "\n",
    "        if self.THRESHOLD != None:\n",
    "            enhanced_image = self.enhance(centered_image)\n",
    "            return enhanced_image\n",
    "        else:\n",
    "            return centered_image\n",
    "\n",
    "    def process_images(self):\n",
    "        \"\"\"Process all images from the dataset.\"\"\"\n",
    "        self.CATEGORIES = self.get_categories()\n",
    "\n",
    "        for category in self.CATEGORIES:\n",
    "            category_path = os.path.join(self.PATH, category)\n",
    "            class_index = self.CATEGORIES.index(category)\n",
    "\n",
    "            for img_name in os.listdir(category_path):\n",
    "                img_path = os.path.join(category_path, img_name)\n",
    "\n",
    "                try:\n",
    "                    image = self.preprocess_image(img_path)\n",
    "                    if image is not None:\n",
    "                        self.x_data.append(image)\n",
    "                        self.y_data.append(class_index)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "        X_Data = np.asarray(self.x_data) / 255.0\n",
    "        Y_Data = np.asarray(self.y_data)\n",
    "        X_Data = X_Data.reshape(-1, self.IMAGE_SIZE, self.IMAGE_SIZE)\n",
    "\n",
    "        return X_Data, Y_Data\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load and return the dataset.\"\"\"\n",
    "\n",
    "        print('Loading Files and Dataset ...')\n",
    "\n",
    "        X_Data, Y_Data = self.process_images()\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_Data, Y_Data, train_size=0.8, stratify=Y_Data,\n",
    "                                                            random_state=0)\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "LABELS = {0: 'Alef',\n",
    "          1: 'Be',\n",
    "          2: 'Pe',\n",
    "          3: 'Te',\n",
    "          4: 'Se',\n",
    "          5: 'Jim',\n",
    "          6: 'Che',\n",
    "          7: 'H',\n",
    "          8: 'Khe',\n",
    "          9: 'Dal',\n",
    "          10: 'Zal',\n",
    "          11: 'Re',\n",
    "          12: 'Ze',\n",
    "          13: 'Zhe',\n",
    "          14: 'Sin',\n",
    "          15: 'Shin',\n",
    "          16: 'Sad',\n",
    "          17: 'Zad',\n",
    "          18: 'Ta',\n",
    "          19: 'Za',\n",
    "          20: 'Ayin',\n",
    "          21: 'Ghayin',\n",
    "          22: 'Fe',\n",
    "          23: 'Ghaf',\n",
    "          24: 'Kaf',\n",
    "          25: 'Gaf',\n",
    "          26: 'Lam',\n",
    "          27: 'Mim',\n",
    "          28: 'Noon',\n",
    "          29: 'Vav',\n",
    "          30: 'He',\n",
    "          31: 'Ye',\n",
    "          32: 'Zero',\n",
    "          33: 'One',\n",
    "          34: 'Two',\n",
    "          35: 'Three',\n",
    "          36: 'Four',\n",
    "          37: 'Five',\n",
    "          38: 'Six',\n",
    "          39: 'Seven',\n",
    "          40: 'Eight',\n",
    "          41: 'Nine',\n",
    "          42: 'Five'}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_PATH = \"Datasets/DS-2 changed\"\n",
    "\n",
    "    dataset_loader = DataLoader(path=DATASET_PATH, image_size=64, shrink=0, padding=15, threshold=None, invert=False)\n",
    "    X_train, y_train, X_test, y_test = dataset_loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 50\n",
    "X_train_resnet = preprocess_input(X_train)\n",
    "X_test_resnet = preprocess_input(X_test)\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "model = Model(inputs=resnet_model.input, outputs=resnet_model.output)\n",
    "X_train_features = model.predict(X_train_resnet)\n",
    "X_test_features = model.predict(X_test_resnet)\n",
    "\n",
    "# Flatten the features\n",
    "X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)\n",
    "X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features_flat)\n",
    "X_test_scaled = scaler.transform(X_test_features_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression GridSearch\n",
    "#>>>>> Accuracy of Logistic before gridsearch: %91\n",
    "lg_params = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"solver\": [\"lbfgs\"],\n",
    "    \"penalty\": [\"l2\", \"none\"],\n",
    "    \"max_iter\": [100, 200],\n",
    "}\n",
    "lg_grid_srch = GridSearchCV(logistic_model, param_grid=lg_params, verbose=2)\n",
    "lg_grid_srch.fit(X_train_scaled, Y_train)\n",
    "print(\"Best Parameters:\", lg_grid_srch.best_params_)\n",
    "##>>>>> Accuracy of Logistic after gridsearch: %91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model (With Best Parameters)\n",
    "logistic_model = LogisticRegression(C=1, max_iter=100, penalty='l2', solver='lbfgs')\n",
    "logistic_model.fit(X_train_scaled, Y_train)\n",
    "y_pred = logistic_model.predict(X_test_scaled)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN GridSearch\n",
    "# >>>>> Accuracy of KNN before gridsearch: %77\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'p': [0.5, 1, 2]\n",
    "}\n",
    "knn_grid_srch = GridSearchCV(knn_model, param_grid=knn_params, verbose=2)\n",
    "knn_grid_srch.fit(X_train_scaled, Y_train)\n",
    "print(\"Best Parameters:\", knn_grid_srch.best_params_)\n",
    "###>>>>> Accuracy of KNN after gridsearch: %81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model (With Best Parameters)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, metric='manhattan', p=0.5, weights='distance')\n",
    "knn_model.fit(X_train_scaled, Y_train)\n",
    "y_pred = knn_model.predict(X_test_scaled)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Gridsearch\n",
    "# >>>>> Accuracy of SVM before gridsearch: %86\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3, 4]\n",
    "}\n",
    "svm_grid_srch = GridSearchCV(svm_model, param_grid=svm_params, verbose=2)\n",
    "svm_grid_srch.fit(X_train_scaled, Y_train)\n",
    "print(\"Best Parameters:\", svm_grid_srch.best_params_)\n",
    "###>>>>> Accuracy of SVM after gridsearch: %91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model\n",
    "svm_model = SVC(kernel='linear', C=0.1, gamma='scale')\n",
    "svm_model.fit(X_train_scaled, Y_train)\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>> Accuracy of Tree before gridsearch: %75\n",
    "tree_params = {\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_depth':['None', 5, 10, 20],\n",
    "    'min_samples_split':[2, 10, 20],\n",
    "    'min_samples_leaf':[1, 5, 10],\n",
    "}\n",
    "tree_grid_srch = GridSearchCV(tree_model, param_grid=tree_params, verbose=2)\n",
    "tree_grid_srch.fit(X_train_scaled, Y_train)\n",
    "print(\"Best Parameters:\", tree_grid_srch.best_params_)\n",
    "###>>>>> Accuracy of Tree after gridsearch: %77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desision Tree\n",
    "tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=1, min_samples_split=2)\n",
    "tree_model.fit(X_train_scaled, Y_train)\n",
    "y_pred = tree_model.predict(X_test_scaled)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
