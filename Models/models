from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.models import Model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

try:
    import tensorflow as tf
    import cv2
    import os
    import pickle
    import numpy as np
    import warnings

    warnings.filterwarnings('ignore')
    print("Libraries Loaded Successfully")
except:
    print("Library not Found !")


class DataLoader:
    def __init__(self, path=r'C:\Users\NoteBook\Desktop\alphabet\DS-3', image_size=64, padding=10, invert=False) -> None:
        self.PATH = path
        self.IMAGE_SIZE = image_size
        self.PADDING = padding
        self.INVERT = invert

        self.image_data = []
        self.x_data = []
        self.y_data = []
        self.labels = []
        self.CATEGORIES = []

        self.list_categories = []

    def get_categories(self):
        for path in (os.listdir(self.PATH)):
            label = path.split("-")[0]
            self.labels.append(label)
            self.list_categories.append(path)
        try:
            self.list_categories = sorted(self.list_categories, key=lambda x: int(x.split("-")[0]))
        except:
            try:
                self.list_categories = sorted(self.list_categories, key=lambda x: int(x))
            except:
                pass

        print("Found Categories:", self.list_categories, '\n')
        return self.list_categories

    def preprocess_image(self, image):
        image_data_temp = cv2.imread(image, cv2.IMREAD_GRAYSCALE)
        image_padded = cv2.copyMakeBorder(image_data_temp, self.PADDING, self.PADDING, self.PADDING, self.PADDING, cv2.BORDER_CONSTANT, value=0)
        if self.INVERT:
            image_padded = 255 - image_padded
        image_temp_resize = cv2.resize(image_padded, (self.IMAGE_SIZE, self.IMAGE_SIZE))
        image_temp_resize = cv2.cvtColor(image_temp_resize, cv2.COLOR_GRAY2RGB)

        return image_temp_resize

    def Process_Images(self):
        """
        Return Numpy array of image
        :return: X_Data, Y_Data
        """
        try:
            self.CATEGORIES = self.get_categories()
            for categories in self.CATEGORIES:
                train_folder_path = os.path.join(self.PATH, categories)
                class_index = self.CATEGORIES.index(categories)

                for img in os.listdir(train_folder_path):
                    new_path = os.path.join(train_folder_path, img)

                    try:
                        image_temp_resize = self.preprocess_image(new_path)

                        if image_temp_resize is not None:
                            self.x_data.append(image_temp_resize)
                            self.y_data.append(class_index)

                    except Exception as e:
                        print(f"Error processing {new_path}: {e}")

            X_Data = np.asarray(self.x_data) / 255.0  # Normalizing the images
            Y_Data = np.asarray(self.y_data)

            return X_Data, Y_Data

        except Exception as e:
            print(f"Failed to run Function Process Image: {e}")
            return None, None

    def load_data(self):
        print('Loading Files and Dataset ...')
        X_Data, Y_Data = self.Process_Images()
        return X_Data, Y_Data


DATASET1 = "Datasets/DS-1"
DATASET2 = "Datasets/DS-2"

LABELS = {0: 'Alef',
          1: 'Be',
          2: 'Pe',
          3: 'Te',
          4: 'Se',
          5: 'Jim',
          6: 'Che',
          7: 'H',
          8: 'Khe',
          9: 'Dal',
          10: 'Zal',
          11: 'Re',
          12: 'Ze',
          13: 'Zhe',
          14: 'Sin',
          15: 'Shin',
          16: 'Sad',
          17: 'Zad',
          18: 'Ta',
          19: 'Za',
          20: 'Ayin',
          21: 'Ghayin',
          22: 'Fe',
          23: 'Ghaf',
          24: 'Kaf',
          25: 'Gaf',
          26: 'Lam',
          27: 'Mim',
          28: 'Noon',
          29: 'Vav',
          30: 'He',
          31: 'Ye',
          32: 'Zero',
          33: 'One',
          34: 'Two',
          35: 'Three',
          36: 'Four',
          37: 'Five',
          38: 'Six',
          39: 'Seven',
          40: 'Eight',
          41: 'Nine',
          42: 'Five'}

if __name__ == "__main__":
    dataset = DataLoader(path=DATASET1, image_size=64, padding=0, invert=True)

    X_Data, Y_Data = dataset.load_data()

    if X_Data is None or Y_Data is None:
        print("Failed to load data correctly")
    else:
        print(f"Data loaded successfully: X_Data shape = {X_Data.shape}, Y_Data shape = {Y_Data.shape}")

# SPLIT DATA
X_train, X_test, Y_train, Y_test = train_test_split(X_Data, Y_Data, test_size=0.2, random_state=42, stratify=Y_Data)

# RESNET 50
X_train_resnet = preprocess_input(X_train)
X_test_resnet = preprocess_input(X_test)
resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))

model = Model(inputs=resnet_model.input, outputs=resnet_model.output)
X_train_features = model.predict(X_train_resnet)
X_test_features = model.predict(X_test_resnet)

# Flatten the features
X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)
X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)

# STANDARD SCALER
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_features_flat)
X_test_scaled = scaler.transform(X_test_features_flat)
#-----------------------------------------------------------------------------------------------------------------------------------------------
# LOGISTIC REGRESSION MODEL
logistic_model = LogisticRegression(C=1, max_iter=100, penalty='l2', solver='lbfgs')
logistic_model.fit(X_train_scaled, Y_train)
y_pred = logistic_model.predict(X_test_scaled)
print(classification_report(Y_test, y_pred))
#>>>>> Logistic: %91
# lg_params = {
#     "C": [0.1, 1, 10, 100],
#     "solver": ["lbfgs"],
#     "penalty": ["l2", "none"],
#     "max_iter": [100, 200],
# }
# lg_grid_srch = GridSearchCV(logistic_model, param_grid=lg_params, verbose=2)
# lg_grid_srch.fit(X_train_scaled, Y_train)
# print("Best Parameters:", lg_grid_srch.best_params_)

##>>>>> Logistic after gridsearch: %91
#------------------------------------------------------------------------------------------------------------------------------------------------
# KNN MODEL
knn_model = KNeighborsClassifier(n_neighbors=5, metric='manhattan', p=0.5, weights='distance')
knn_model.fit(X_train_scaled, Y_train)
y_pred = knn_model.predict(X_test_scaled)
print(classification_report(Y_test, y_pred))
###>>>>> knn: %77
# knn_params = {
#     'n_neighbors': [3, 5, 7, 9, 11],
#     'weights': ['uniform', 'distance'],
#     'metric': ['euclidean', 'manhattan', 'minkowski'],
#     'p': [0.5, 1, 2]
# }
# knn_grid_srch = GridSearchCV(knn_model, param_grid=knn_params, verbose=2)
# knn_grid_srch.fit(X_train_scaled, Y_train)
# print("Best Parameters:", knn_grid_srch.best_params_)

###>>>>> knn after gridsearch: %81
#------------------------------------------------------------------------------------------------------------------------------------------------
# SVM MODEL
svm_model = SVC(kernel='linear', C=0.1, gamma='scale')
svm_model.fit(X_train_scaled, Y_train)
y_pred = svm_model.predict(X_test_scaled)
print(classification_report(Y_test, y_pred))
###>>>>> SVM: %86
# svm_params = {
#     'C': [0.1, 1, 10, 100],
#     'kernel': ['linear', 'rbf', 'poly'],
#     'gamma': ['scale', 'auto'],
#     'degree': [2, 3, 4]
# }
# svm_grid_srch = GridSearchCV(svm_model, param_grid=svm_params, verbose=2)
# svm_grid_srch.fit(X_train_scaled, Y_train)
# print("Best Parameters:", svm_grid_srch.best_params_)

###>>>>> SVM after gridsearch: %91
#-----------------------------------------------------------------------------------------------------------------------------------------------
# DESISION TREE
tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=1, min_samples_split=2)
tree_model.fit(X_train_scaled, Y_train)
y_pred = tree_model.predict(X_test_scaled)
print(classification_report(Y_test, y_pred))
###>>>>> DESISION TREE: %75
# tree_params = {
#     'criterion':['gini', 'entropy'],
#     'max_depth':['None', 5, 10, 20],
#     'min_samples_split':[2, 10, 20],
#     'min_samples_leaf':[1, 5, 10],
# }
# tree_grid_srch = GridSearchCV(tree_model, param_grid=tree_params, verbose=2)
# tree_grid_srch.fit(X_train_scaled, Y_train)
# print("Best Parameters:", tree_grid_srch.best_params_)

###>>>>> DESISION TREE after gridsearch: %77
